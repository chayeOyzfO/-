import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, AutoConfig
import numpy as np
import random

# è®¾å¤‡æ£€æµ‹å’Œè®¾ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

if torch.cuda.is_available():
    print("ğŸ¯ ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ")
    # GPUä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True  # åŠ é€Ÿå·ç§¯å±‚
    torch.backends.cudnn.deterministic = False  # ä¸ºäº†é€Ÿåº¦ç‰ºç‰²å¯é‡å¤æ€§
else:
    print("âš¡ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
np.random.seed(42)
random.seed(42)

class PolicyDataset(Dataset):
    def __init__(self, texts, intents=None, entities=None, tokenizer=None, max_length=128):
        self.texts = texts
        self.intents = intents
        self.entities = entities
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # æ„å›¾æ ‡ç­¾æ˜ å°„
        self.intent_labels = [
            'query_subsidy', 'application_process', 'product_scope',
            'qualification_check', 'document_requirements', 'deadline_query',
            'regional_policy', 'appeal_process', 'policy_comparison', 'other'
        ]
        self.intent2id = {label: idx for idx, label in enumerate(self.intent_labels)}
        self.id2intent = {idx: label for label, idx in self.intent2id.items()}
        
        # å®ä½“æ ‡ç­¾æ˜ å°„
        self.entity_labels = ['O', 'B-SUBSIDY', 'I-SUBSIDY', 'B-PRODUCT', 'I-PRODUCT',
                             'B-LOCATION', 'I-LOCATION', 'B-TIME', 'I-TIME',
                             'B-CONDITION', 'I-CONDITION', 'B-DOCUMENT', 'I-DOCUMENT']
        self.entity2id = {label: idx for idx, label in enumerate(self.entity_labels)}
        self.id2entity = {idx: label for label, idx in self.entity2id.items()}
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # Tokenizeæ–‡æœ¬
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        output = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }
        
        # æ·»åŠ æ„å›¾æ ‡ç­¾
        if self.intents is not None:
            intent = self.intents[idx]
            intent_id = self.intent2id.get(intent, self.intent2id['other'])
            output['intent_labels'] = torch.tensor(intent_id, dtype=torch.long)
        
        # æ·»åŠ å®ä½“æ ‡ç­¾
        if self.entities is not None:
            entity_tags = self.entities[idx]
            # å°†å®ä½“æ ‡ç­¾è½¬æ¢ä¸ºID
            entity_ids = []
            for i, tag in enumerate(entity_tags):
                if i < self.max_length:
                    entity_ids.append(self.entity2id.get(tag, self.entity2id['O']))
            
            # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            while len(entity_ids) < self.max_length:
                entity_ids.append(self.entity2id['O'])
                
            output['entity_labels'] = torch.tensor(entity_ids[:self.max_length], dtype=torch.long)
        
        return output

class PolicyMultiTaskModel(nn.Module):
    def __init__(self, model_name='hfl/chinese-roberta-wwm-ext', num_intents=10, num_entities=13):
        super().__init__()
        self.config = AutoConfig.from_pretrained(model_name)
        self.encoder = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.config.hidden_size
        
        # æ„å›¾åˆ†ç±»å™¨
        self.intent_classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_intents)
        )
        
        # å®ä½“è¯†åˆ«å™¨
        self.entity_recognizer = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_entities)
        )
        
        # æŸå¤±å‡½æ•°
        self.intent_loss_fn = nn.CrossEntropyLoss()
        self.entity_loss_fn = nn.CrossEntropyLoss(ignore_index=0)
        
    def forward(self, input_ids, attention_mask, intent_labels=None, entity_labels=None):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        pooled_output = outputs.pooler_output
        
        # æ„å›¾åˆ†ç±»
        intent_logits = self.intent_classifier(pooled_output)
        
        # å®ä½“è¯†åˆ«
        entity_logits = self.entity_recognizer(sequence_output)
        
        loss = 0
        if intent_labels is not None and entity_labels is not None:
            intent_loss = self.intent_loss_fn(intent_logits, intent_labels)
            entity_loss = self.entity_loss_fn(
                entity_logits.view(-1, entity_logits.size(-1)), 
                entity_labels.view(-1)
            )
            loss = intent_loss + 0.8 * entity_loss
        
        return {
            'loss': loss,
            'intent_logits': intent_logits,
            'entity_logits': entity_logits
        }

class PolicyTrainer:
    def __init__(self, model, train_loader, val_loader, learning_rate=2e-5):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.model.to(self.device)
        
        # å¦‚æœæœ‰å¤šGPUï¼Œä½¿ç”¨DataParallel
        if torch.cuda.device_count() > 1:
            print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
            self.model = nn.DataParallel(self.model)
        
        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=10)
        
    def train_epoch(self, epoch):
        self.model.train()
        total_loss = 0
        intent_correct = 0
        intent_total = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
            input_ids = batch['input_ids'].to(self.device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
            intent_labels = batch['intent_labels'].to(self.device, non_blocking=True)
            entity_labels = batch['entity_labels'].to(self.device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                intent_labels=intent_labels,
                entity_labels=entity_labels
            )
            
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # è®¡ç®—æ„å›¾å‡†ç¡®ç‡
            intent_preds = torch.argmax(outputs['intent_logits'], dim=1)
            intent_correct += (intent_preds == intent_labels).sum().item()
            intent_total += intent_labels.size(0)
            
            if batch_idx % 10 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}, GPU Mem: {gpu_memory:.2f}GB')
                else:
                    print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}')
        
        avg_loss = total_loss / len(self.train_loader)
        intent_acc = intent_correct / intent_total
        return avg_loss, intent_acc
    
    def validate(self):
        self.model.eval()
        total_loss = 0
        intent_correct = 0
        intent_total = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                input_ids = batch['input_ids'].to(self.device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
                intent_labels = batch['intent_labels'].to(self.device, non_blocking=True)
                entity_labels = batch['entity_labels'].to(self.device, non_blocking=True)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    intent_labels=intent_labels,
                    entity_labels=entity_labels
                )
                
                total_loss += outputs['loss'].item()
                intent_preds = torch.argmax(outputs['intent_logits'], dim=1)
                intent_correct += (intent_preds == intent_labels).sum().item()
                intent_total += intent_labels.size(0)
        
        avg_loss = total_loss / len(self.val_loader)
        intent_acc = intent_correct / intent_total
        return avg_loss, intent_acc
    
    def train(self, epochs=3):
        print("å¼€å§‹è®­ç»ƒæ”¿ç­–å’¨è¯¢æ™ºèƒ½ä½“...")
        
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            train_loss, train_acc = self.train_epoch(epoch)
            val_loss, val_acc = self.validate()
            
            self.scheduler.step()
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}')
            print(f'  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}')
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                # ä¿å­˜æ¨¡å‹æ—¶ç§»é™¤DataParallelåŒ…è£…
                model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model_to_save.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'loss': best_val_loss,
                    'device': str(self.device)
                }, 'best_policy_model.pth')
                print(f'  ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° best_policy_model.pth')
            
            print('-' * 60)

# æ•°æ®ç”Ÿæˆå‡½æ•°
def generate_policy_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ”¿ç­–å’¨è¯¢è®­ç»ƒæ•°æ®"""
    
    # ç¤ºä¾‹é—®é¢˜å’Œå¯¹åº”çš„æ„å›¾
    intent_data = [
        # è¡¥è´´æŸ¥è¯¢
        ("æ±½è½¦ä»¥æ—§æ¢æ–°è¡¥è´´å¤šå°‘é’±ï¼Ÿ", "query_subsidy"),
        ("å®¶ç”µè¡¥è´´æ ‡å‡†æ˜¯å¤šå°‘ï¼Ÿ", "query_subsidy"),
        ("æ‰‹æœºä»¥æ—§æ¢æ–°èƒ½è¡¥è´´å¤šå°‘ï¼Ÿ", "query_subsidy"),
        ("æ–°èƒ½æºæ±½è½¦è¡¥è´´æ”¿ç­–", "query_subsidy"),
        ("ä»¥æ—§æ¢æ–°è¡¥è´´é‡‘é¢", "query_subsidy"),
        
        # ç”³è¯·æµç¨‹
        ("æ€ä¹ˆç”³è¯·å®¶ç”µè¡¥è´´ï¼Ÿ", "application_process"),
        ("æ±½è½¦ä»¥æ—§æ¢æ–°ç”³è¯·æ­¥éª¤", "application_process"),
        ("è¡¥è´´ç”³è¯·éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ", "application_process"),
        ("çº¿ä¸Šç”³è¯·æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ", "application_process"),
        ("ç”³è¯·è¡¥è´´çš„å…·ä½“æµç¨‹", "application_process"),
        
        # äº§å“èŒƒå›´
        ("å“ªäº›æ‰‹æœºå¯ä»¥å‚ä¸ä»¥æ—§æ¢æ–°ï¼Ÿ", "product_scope"),
        ("æ”¯æŒä»¥æ—§æ¢æ–°çš„å®¶ç”µç±»å‹", "product_scope"),
        ("å“ªäº›æ±½è½¦å“ç‰Œå‚ä¸æ´»åŠ¨ï¼Ÿ", "product_scope"),
        ("æ•°ç äº§å“åŒ…æ‹¬å“ªäº›ï¼Ÿ", "product_scope"),
        ("å‚ä¸ä»¥æ—§æ¢æ–°çš„äº§å“èŒƒå›´", "product_scope"),
        
        # èµ„æ ¼æ£€æŸ¥
        ("æˆ‘ç¬¦åˆè¡¥è´´æ¡ä»¶å—ï¼Ÿ", "qualification_check"),
        ("ç”³è¯·éœ€è¦ä»€ä¹ˆèµ„æ ¼ï¼Ÿ", "qualification_check"),
        ("å¤–åœ°æˆ·å£å¯ä»¥ç”³è¯·å—ï¼Ÿ", "qualification_check"),
        ("ä¼ä¸šå¯ä»¥å‚ä¸å—ï¼Ÿ", "qualification_check"),
        ("ä¸ªäººç”³è¯·æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ", "qualification_check"),
        
        # ææ–™è¦æ±‚
        ("æˆ‘éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ", "document_requirements"),
        ("ç”³è¯·éœ€è¦å“ªäº›è¯ä»¶ï¼Ÿ", "document_requirements"),
        ("è¦æäº¤ä»€ä¹ˆè¯æ˜æ–‡ä»¶ï¼Ÿ", "document_requirements"),
        ("ææ–™æ¸…å•æœ‰å“ªäº›ï¼Ÿ", "document_requirements"),
        ("éœ€è¦å‡†å¤‡å“ªäº›ç”³è¯·ææ–™ï¼Ÿ", "document_requirements"),
        
        # æˆªæ­¢æ—¶é—´
        ("ç”³è¯·æˆªæ­¢åˆ°ä»€ä¹ˆæ—¶å€™ï¼Ÿ", "deadline_query"),
        ("æ´»åŠ¨æŒç»­åˆ°å“ªå¤©ï¼Ÿ", "deadline_query"),
        ("è¡¥è´´æ”¿ç­–æœ‰æ•ˆæœŸ", "deadline_query"),
        ("ä»€ä¹ˆæ—¶å€™æˆªæ­¢ç”³è¯·ï¼Ÿ", "deadline_query"),
        ("æ”¿ç­–æ‰§è¡Œåˆ°ä½•æ—¶ï¼Ÿ", "deadline_query"),
        
        # åœ°åŒºæ”¿ç­–
        ("åŒ—äº¬åœ°åŒºçš„è¡¥è´´æ”¿ç­–", "regional_policy"),
        ("ä¸Šæµ·ä»¥æ—§æ¢æ–°æ ‡å‡†", "regional_policy"),
        ("å¹¿å·æœ‰ä»€ä¹ˆç‰¹æ®Šæ”¿ç­–ï¼Ÿ", "regional_policy"),
        ("æ·±åœ³åœ°åŒºçš„è¡¥è´´", "regional_policy"),
    ]
    
    texts = [item[0] for item in intent_data]
    intents = [item[1] for item in intent_data]
    
    # ç”Ÿæˆå®ä½“æ ‡ç­¾
    entities = []
    for text in texts:
        entity_tags = ['O'] * len(text)
        
        # ç®€å•è§„åˆ™åŒ¹é…å®ä½“
        entity_keywords = {
            'æ±½è½¦': 'B-PRODUCT',
            'å®¶ç”µ': 'B-PRODUCT', 
            'æ‰‹æœº': 'B-PRODUCT',
            'æ•°ç ': 'B-PRODUCT',
            'æ–°èƒ½æº': 'B-PRODUCT',
            'è¡¥è´´': 'B-SUBSIDY',
            'åŒ—äº¬': 'B-LOCATION',
            'ä¸Šæµ·': 'B-LOCATION',
            'å¹¿å·': 'B-LOCATION',
            'æ·±åœ³': 'B-LOCATION',
            'ææ–™': 'B-DOCUMENT',
            'è¯ä»¶': 'B-DOCUMENT',
            'æ–‡ä»¶': 'B-DOCUMENT',
            'æ¡ä»¶': 'B-CONDITION',
            'èµ„æ ¼': 'B-CONDITION',
        }
        
        for keyword, label in entity_keywords.items():
            if keyword in text:
                idx = text.index(keyword)
                entity_tags[idx] = label
                # æ ‡è®°åç»­å­—ç¬¦
                for i in range(idx + 1, min(idx + len(keyword), len(text))):
                    if i < len(entity_tags):
                        entity_tags[i] = label.replace('B-', 'I-')
        
        entities.append(entity_tags)
    
    return texts, intents, entities

# æ¨ç†ç±»
class PolicyInference:
    def __init__(self, model_path=None):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')
        
        # åŠ è½½æ¨¡å‹
        self.model = PolicyMultiTaskModel()
        if model_path and torch.cuda.is_available():
            checkpoint = torch.load(model_path, map_location='cuda')
        elif model_path:
            checkpoint = torch.load(model_path, map_location='cpu')
        else:
            checkpoint = None
            
        if checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        # æ ‡ç­¾æ˜ å°„
        self.intent_labels = ['query_subsidy', 'application_process', 'product_scope',
                             'qualification_check', 'document_requirements', 'deadline_query',
                             'regional_policy', 'appeal_process', 'policy_comparison', 'other']
        self.entity_labels = ['O', 'B-SUBSIDY', 'I-SUBSIDY', 'B-PRODUCT', 'I-PRODUCT',
                             'B-LOCATION', 'I-LOCATION', 'B-TIME', 'I-TIME',
                             'B-CONDITION', 'I-CONDITION', 'B-DOCUMENT', 'I-DOCUMENT']
    
    def predict(self, text):
        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        
        # è·å–æ„å›¾é¢„æµ‹
        intent_logits = outputs['intent_logits']
        intent_pred = torch.argmax(intent_logits, dim=1)
        intent_label = self.intent_labels[intent_pred.item()]
        confidence = torch.softmax(intent_logits, dim=1).max().item()
        
        # è·å–å®ä½“é¢„æµ‹
        entity_logits = outputs['entity_logits']
        entity_preds = torch.argmax(entity_logits, dim=2)
        
        # æå–å®ä½“
        entities = []
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        for i, (token, pred_idx) in enumerate(zip(tokens, entity_preds[0])):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
            entity_label = self.entity_labels[pred_idx.item()]
            if entity_label != 'O':
                entities.append({
                    'word': token,
                    'entity': entity_label,
                    'position': i
                })
        
        return {
            'text': text,
            'intent': intent_label,
            'confidence': confidence,
            'entities': entities
        }

def main():
    # æ£€æŸ¥GPUçŠ¶æ€
    print("=== è®¾å¤‡ä¿¡æ¯ ===")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    print("================")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    texts, intents, entities = generate_policy_data()
    print(f"ç”Ÿæˆ {len(texts)} æ¡è®­ç»ƒæ•°æ®")
    
    # åˆå§‹åŒ–tokenizer
    tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = PolicyDataset(texts, intents, entities, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)
    
    # åˆ›å»ºéªŒè¯é›†ï¼ˆä½¿ç”¨éƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼‰
    val_size = min(8, len(texts) // 4)
    val_dataset = PolicyDataset(texts[:val_size], intents[:val_size], entities[:val_size], tokenizer)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = PolicyMultiTaskModel()
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°: æ€»å…± {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
    
    # å¼€å§‹è®­ç»ƒ
    trainer = PolicyTrainer(model, train_loader, val_loader)
    trainer.train(epochs=3)
    
    # æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
    print("\n=== æµ‹è¯•æ¨¡å‹ ===")
    inference = PolicyInference('best_policy_model.pth')
    
    test_questions = [
        "æ±½è½¦ä»¥æ—§æ¢æ–°è¡¥è´´å¤šå°‘é’±ï¼Ÿ",
        "æ€ä¹ˆç”³è¯·å®¶ç”µè¡¥è´´ï¼Ÿ",
        "å“ªäº›æ‰‹æœºå¯ä»¥å‚ä¸æ´»åŠ¨ï¼Ÿ",
        "åŒ—äº¬åœ°åŒºçš„è¡¥è´´æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ç”³è¯·éœ€è¦ä»€ä¹ˆææ–™ï¼Ÿ"
    ]
    
    for question in test_questions:
        result = inference.predict(question)
        print(f"\né—®é¢˜: {result['text']}")
        print(f"æ„å›¾: {result['intent']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        if result['entities']:
            print(f"å®ä½“: {[entity['word'] for entity in result['entities']]}")
        else:
            print("å®ä½“: æ— ")

if __name__ == "__main__":
    main()
    print("\nè®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º 'best_policy_model.pth'")